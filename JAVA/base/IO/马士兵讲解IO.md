ref：https://www.bilibili.com/video/BV11K4y1C7rm?from=search&seid=12483957696353213611

- redis:epoll
- nginx:epoll
- Netty:epoll

操纵系统内核：

电脑一开机：第一个应用程序内核先进入内存，进入到内存之后，注册一个GDT全局描述符表。这个表划分空间的概念。 

- 用户空间
- 内核空间：它管硬件（硬件包括磁盘和网卡）

为什么要划分这两个空间：保护内核不会被黑掉

我们用户的程序app是不能访问内核的数据（kernel，防止程序修改内核）

但是我们用户的app想访问磁盘的数据怎么办？

所以内核提供了一个方法：syscall （一类方法，比如读写磁盘），但是由于保护模式，用户程序不能直接调用内核的方法。所以出现了 **系统中断**

- 软中断：
- 硬中断：键盘按键中断，鼠标移动中断cpu，触发系统内核的callback回调函数。

程序app走软中断，程序想要执行IO操作，就必须调用内核。

<img src="%E9%A9%AC%E5%A3%AB%E5%85%B5%E8%AE%B2%E8%A7%A3IO.assets/image-20210901185343443.png" alt="image-20210901185343443" style="zoom: 67%;" />

BIO socket程序执行的步骤

1. 调用socket系统调用，得到一个fd5文件描述符（这个文件描述符不代表这个socket）
2. 绑定一个端口号8090
3. listen监听，将我们的fd5变成监听状态
4. 调用accept 监听有没有一个客户端连接进来，阻塞状态，直到有客户端连接进来 fd5 = fd6
5. 系统调用read，recvfrom 从客户端fd6接收数据

多客户端的连接问题：BIO（阻塞连接）多线程解决方案

此时有一个问题，当我们接受fd6来的数据的时候，有另外一个客户端想要连接进来，但是不能连，因为主线程正在处于阻塞状态。怎么解决呢。linux是这样的。将这个阻塞的线程克隆一份，使用另外一个线程去阻塞等待客户端的输入。而我的主线程继续循环等待另外一个客户端的连接。**每个线程对应一个客户端的连接**

这个解决方法的弊端：创建线程会发生系统调用clone出来线程来阻塞，软中断。线程消耗资源，内存栈可以M，cpu调度也会浪费。阻塞的特征是根本原因。

<img src="%E9%A9%AC%E5%A3%AB%E5%85%B5%E8%AE%B2%E8%A7%A3IO.assets/image-20210901194540689.png" alt="image-20210901194540689" style="zoom: 50%;" />

NIO socket程序执行的步骤

1. 调用socket系统调用，得到一个fd5文件描述符（这个文件描述符不代表这个socket）
2. 绑定一个端口号8090
3. listen监听，将我们的fd5变成监听状态
4. 调用accept 监听有没有一个客户端连接进来，阻塞状态，直到有客户端连接进来 fd5 = fd6
5. 系统调用read，recvfrom 从客户端fd6接收数据

多客户端的连接问题：NIO（非阻塞连接）单线程解决方案

NIO有两个层级：

- app层级的NIO是new 新的体系
- 内核级别的是非阻塞的意思

当我们接受fd6来的数据的时候，有另外一个客户端想要连接进来，读取线程的数据不会阻塞，所以可以有很多的线程都连接进来。一个线程连接10000个客户端，连接一个客户端就有一个文件描述符。要想从这些客户端接受数据，就需要系统调用recvfrom。有时候只有一两个客户端想要发送数据。但是我得把所有的客户端的描述符都调用。也就是每循环内会有O（n）10000次的系统调用。但是大部分的系统调用都浪费了。白调用了。

<img src="%E9%A9%AC%E5%A3%AB%E5%85%B5%E8%AE%B2%E8%A7%A3IO.assets/image-20210901194609155.png" alt="image-20210901194609155" style="zoom:50%;" />

### 多路复用select

1. 程序首先做一个系统调用select（多路复用器，返回的是状态，根据这些状态来读取数据，同步）
2. 一次系统调用传入10000个文件描述符发送给内核
3. 内核通过这次的系统调用给你返回n个可用的状态
4. 根据返回的信息，我们得知需要recvfrom哪几个客户端发来的数据 复杂度O（m）

<img src="%E9%A9%AC%E5%A3%AB%E5%85%B5%E8%AE%B2%E8%A7%A3IO.assets/image-20210901200247458.png" alt="image-20210901200247458" style="zoom: 67%;" />

问题：

- 每次传递10000个数据给select，select还得将这么多的数据传递给系统内核。数据传输量大。

- 虽然只有一次系统调用，但是这次的系统调用需要遍历这些文件描述符。遍历的复杂度也是O（n）级别的

  

### 多路复用：epoll

- 创建进程fd5文件描述符
- 将fd5文件描述符绑定8090端口号
- 监听fd5文件描述符（监听8090端口号）
- 调用epoll_create，返回一个文件描述符fd，开辟一个内核的区域fd8，fd8（句柄）就代表这个空间
- 创建了空间之后会执行epoll_ctl（fd8，add，fd5，accept）将监听文件fd5放到fd8空间里面，想接受客户端连接 。无论想要添加多少个客户端，只需要这一个fd5监听文件即可。
- 如果一个客户端想要连接，fd5文件描述符就会有一个连接事件，fd5就会挪到另外一个区域，服务器就会在这个区域等待客户端传输的数据。
- epoll_waite（fd8）
- 当客户端连接以后，fd5是服务器就会接收它，accept，就会返回一个客户端连接fd6
- 当客户端得到以后，每个客户端生命周期里面最少有一次epoll_ctl，把它自己添加到fd8内存空间里面。
- 比如说这个事件fd6是读写。想要读写数据必须挪到另外一个区域，这个时候就会用到 **事件驱动**。
- （到此避免了内核主动遍历描述文件）

每来一个客户端都会放到fd8内核内存空间里面堆积

<img src="%E9%A9%AC%E5%A3%AB%E5%85%B5%E8%AE%B2%E8%A7%A3IO.assets/image-20210901204931034.png" alt="image-20210901204931034" style="zoom: 67%;" />

