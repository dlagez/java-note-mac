参数的改变：

- 使用了 batch normalization 发现收敛的速度变快了，64或者128都会使得网络的收敛速度加快，就是loss降低的比较快。但是我测试把batch-size直接调整比较大的数字时比如500，网络收敛的速度反而稍微慢了一点。不是很明显

```
[Epoch 3/20] [Batch 529/938] [D loss: 1.051645, acc: 92%] [G loss: 1.201317]
[Epoch 3/20] [Batch 530/938] [D loss: 1.096956, acc: 89%] [G loss: 1.269351]
[Epoch 3/20] [Batch 531/938] [D loss: 1.119168, acc: 93%] [G loss: 1.161988]
```

每次训练还是很话费时间的，只是训练20个epochs 就需要大概需要半个小时。

- 损失刚开始下降的很平稳， 只是差不多四到五个epochs时，G网络和D网络的损失就一直在一附近徘徊，并没有继续下降，像是遇到了局部最小值一样。

- 我尝试过增加学习率，降低学习率，后来尝试了下用GD用不同的学习率，也没有发现非常好的效果，发现损失还是降到1之后就在1附近徘徊，